{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# McNemar's test for statistical significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from statsmodels.stats.contingency_tables import mcnemar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000 30000 30000 30000 30000\n"
     ]
    }
   ],
   "source": [
    "actual = pd.read_csv(\"actual_test_results.csv\",index_col=0)\n",
    "actual = actual['lang'].map({\"eng\": 0, \"deu\": 1, \"spa\": 2, \"fra\": 3, \"por\": 4, \"ita\": 5})\n",
    "actual = list(actual)\n",
    "\n",
    "baseline = list([0]*30000)\n",
    "baseline\n",
    "\n",
    "ann = pd.read_csv(\"ann_test_results_final.csv\",index_col=0)\n",
    "ann = list(ann['ann_test_result'])\n",
    "\n",
    "\n",
    "svm = pd.read_csv(\"svm_test_results_final.csv\",sep='\\t')\n",
    "svm = svm['0.2'].map({\"b'eng'\": 0, \"b'deu'\": 1, \"b'spa'\": 2, \"b'fra'\": 3, \"b'por'\": 4, \"b'ita'\": 5})\n",
    "svm = list(svm)\n",
    "\n",
    "ngram = pd.read_csv(\"ngram_test_results_final.csv\",sep=\",\")\n",
    "ngram = list(ngram['y_pred'])\n",
    "\n",
    "print(len(actual), len(baseline),len(ann),len(svm),len(ngram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1698\n",
      "0.9821\n",
      "0.9851666666666666\n",
      "0.9985666666666667\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(actual,baseline,6))\n",
    "print(accuracy_score(actual,ann))\n",
    "print(accuracy_score(actual,svm))\n",
    "print(accuracy_score(actual,ngram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nemar(actual, r1,r2):\n",
    "    t1 = [True if actual[i] == r1[i] else False for i in range(30000)]\n",
    "    t2 = [True if actual[i] == r2[i] else False for i in range(30000)]\n",
    "    table = confusion_matrix(t1,t2)\n",
    "    mc = mcnemar(table, exact=False)\n",
    "    print(mc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pvalue      0.0\n",
      "statistic   24295.218035268605\n",
      "None\n",
      "pvalue      0.0\n",
      "statistic   24379.267348518806\n",
      "None\n",
      "pvalue      0.0\n",
      "statistic   24855.0019703245\n",
      "None\n",
      "pvalue      1.6176749166746775e-06\n",
      "statistic   23.002777777777776\n",
      "None\n",
      "pvalue      8.21823005004125e-99\n",
      "statistic   445.14468864468864\n",
      "None\n",
      "pvalue      5.264370436186697e-78\n",
      "statistic   349.56739130434784\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(nemar(actual,baseline,ann))\n",
    "print(nemar(actual,baseline,svm))\n",
    "print(nemar(actual,baseline,ngram))\n",
    "\n",
    "print(nemar(actual,ann,svm))\n",
    "print(nemar(actual,ann,ngram))\n",
    "\n",
    "print(nemar(actual,svm, ngram))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
